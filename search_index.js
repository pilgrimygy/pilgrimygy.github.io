var documenterSearchIndex = {"docs":
[{"location":"blogroll/#友链","page":"🔗 友链","title":"🔗 友链","text":"","category":"section"},{"location":"blogroll/","page":"🔗 友链","title":"🔗 友链","text":"last_update=\"2021-08-16\"\ncreate=\"2021-08-12\"","category":"page"},{"location":"blogroll/","page":"🔗 友链","title":"🔗 友链","text":"欢迎在评论区或者私下我提供友链~","category":"page"},{"location":"blogroll/#[Tianjun](https://tianjun.me/)","page":"🔗 友链","title":"Tianjun","text":"","category":"section"},{"location":"blogroll/","page":"🔗 友链","title":"🔗 友链","text":"2021年做OSPP项目时候的mentor。人非常好，帮助了我很多还请我吃过饭😏","category":"page"},{"location":"blogroll/","page":"🔗 友链","title":"🔗 友链","text":"","category":"page"},{"location":"essays/uncertainty_in_offline_rl/#离线强化学习中的不确定性","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"","category":"section"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"last_update=\"2021-08-16\"\ncreate=\"2021-08-12\"","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"不确定性（uncertainty）在机器学习、CV里研究的比较多。在训练好的模型中，我们给出一个输入，模型会给我们一个输出。但是有时候我们不只是要一个输出，我们还需要输出模型预测出该输出的置信度，即不确定性，辅助我们更好的进行决策。这样对于OOD（Out-of-distribution）的检测很有帮助。","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"论文[1]在CV领域把不确定性分为了两类：","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"Aleatoric uncertainty，也就是随机不确定性，也可以称为数据不确定性。它来源于训练数据本身的噪声，是随机而且固定的，是无法通过增加数据减小的。\nEpistemic uncertainty，也就是认知不确定性，也可以称为模型不确定性。这种主要是由于模型预测不好导致的，可以通过增加数据去解决这个问题。","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"在离线强化学习中，我们主要研究模型不确定性。但是计算不确定性函数并在其上运行RL算法往往与策略约束方法没有太大区别，所以需要一些其他的方法：","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"在Model-based的强化学习中，使用的方式是（待更新）\n在Model-free的强化学习中，主要是用集成的方式和MC Dropout的方式来对Q函数的不确定性进行建模。","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"具体来说，我们可以把策略的更新写为：","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"pi_k+1 = argmax_pi mathbbE_asim mathcalDmathbbE_asim pi(as)mathbbE_Q^pi_k+1sim mathcalP_mathcalD(Q^pi)Q^pi_k+1(sa) - alpha Unc(mathcalP_mathcalD(Q^pi))","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"在这里，mathcalP_mathcalD是给定数据集，Q函数的集合，而Unc就是Q集合的不确定值。这样，在不确定值大的时候策略趋于保守。","category":"page"},{"location":"essays/uncertainty_in_offline_rl/#集成方法","page":"离线强化学习中的不确定性","title":"集成方法","text":"","category":"section"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"这个方法来自文献[2]，是发表在ICML'20上的一篇文章。","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"它的思想相当简单，就是同时学习N个不同参数的Q网络，每次更新时候选择一个它们的凸组合，然后组合得到新的一个Q值：","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"Q(sa)=sum_i=1^N alpha_iQ_phi_i(sa) sum_i=1^Nalpha_i=1","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"<img src=\"REM.png\" width = \"300\" align=center />","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"其余的更新方式就类似于普通的DQN。这个方案比起之前提出的独立学习多个Q网络的，然后选择的时候投票选择的Bootstrap ensemble方法来说更优。因为Bootstrap ensemble方法虽然是利用到了不确定性，但是这个集成方案中，实际上Q值很容易趋同，缺少多样性，导致不确定性估计的错误。不确定性的估计本身也需要策略具有一定的多样性。","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"我复现了这个方法（代码），但是实际效果并不好。仔细研读论文发现一个问题：可能要在覆盖面较广的专家数据集上训练才会有比较好的效果。","category":"page"},{"location":"essays/uncertainty_in_offline_rl/#Monte-Carlo-Dropout方法","page":"离线强化学习中的不确定性","title":"Monte Carlo Dropout方法","text":"","category":"section"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"这个方法来自文献[3]，是发表在ICML'21上的一篇spotlight文章。","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"建模不确定性需要用到贝叶斯深度学习。在离线强化学习中，我们令X=(sa)，即状态动作对；而Y表示真实的Q值；theta是Q函数的参数。按照贝叶斯的角度，我们需要最大化p(thetaXY)：","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"p(thetaX Y) = fracP(YXtheta)P(theta)P(YX)","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"但是P(Y|X)是无法获得的。这里是使用Dropout作为变分推断的近似（论文[4]中详细的说明了这个过程，后续补充），具体来说，在初始化Q网络的时候进行Dropout，然后在测试和训练的时候都Dropout。我们采样T次，不确定性可以用Q的近似预测方差表示为：","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"mathbbVQ(sa) approx sigma^2 + frac1Tsum_i=1^ThatQ_t(sa)^ThatQ_t(sa) - mathbbEhatQ(sa)^TmathbbEhatQ(sa)","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"其中，sigma项是数据不确定性，后面两项是模型不确定性，UWAC（Uncertainty Weighted Actor-Critic）主要使用后两项来权衡OOD样本的影响。具体来说，它对训练的策略进行加权：","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"pi(as) = fracbetamathbbVQ_0^pi(sa)pi(as)Z(s) \nZ(s) = int_a fracbetamathbbVQ_0^pi(sa)pi(as) da","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"这里Q_0^pi(sa)指的是上一轮迭代得到的Q值。文章给出了理论证明更新这样的策略对OOD训练样本产生更好的收敛特性（理论部分待更新）。此外，策略的变化也会导致Actor和Critic中的损失函数的变化。","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"Critic的损失函数：","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"mathcalL(Q_theta)=mathbbE_(ssa)sim DmathbbE_asim piBigfracbetamathbbVQ_0^pi(sa)*Err(sasa)^2Big","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"这个Err函数就是标准的当前Q值与目标Q值的损失。Actor的损失函数：","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"mathcalL(pi)=-mathbbE_asim piBigfracbetamathbbVQ_0^pi(sa)Q_theta(sa)Big","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"这个损失函数直观地降低了在OOD样本上最大化Q函数的概率，进一步阻止了预测的Q值函数爆炸。","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"UWAC是在基线算法BEAR^5上进行改进的，伪代码如下：","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"<img src=\"UWAC.png\" width = \"400\" align=center />","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"UWAC的做法是很直观的，并且对原基线算法的改动很小（红色标记）。UWAC用dropout的方式得到多个Q值。如果本身我们能对这个样本进行准确估计的时候，这个方差就会比较小，从而使得策略更偏向于选择这个样本对应的动作。如果不能准确估计则相反。","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"实验结果如下：","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"<img src=\"UWACexp.png\" width = \"800\" align=center />","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"从这个实验结果我们能够看出一些问题：UWAC在专家数据集上的表现很好，超过了现有的其他方法。但是在次优甚至随机的数据集中表现就和他的基线算法BEAR区别不太大。","category":"page"},{"location":"essays/uncertainty_in_offline_rl/#总结","page":"离线强化学习中的不确定性","title":"总结","text":"","category":"section"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"除了这些方法，已经用于强化学习的建模不确定性的方法还有不少，比如分布式强化学习。但是，不确定性的估计本身是依赖于数据质量的，而这样加权的学习方法只能解决策略估计的准确度问题，但是却很难解决策略估计不好的问题。也就是说，不确定性本身是一个很重要的指标，但是它的使用（以目前方法来说）并不能从根本上解决训练出策略不足够优的问题（就算估计的很准，策略不好那又能怎么样呢？）。如何使得这样的方法在次优甚至随机的数据集上都能有较好的表现是一个值得研究的问题。","category":"page"},{"location":"essays/uncertainty_in_offline_rl/#参考文献","page":"离线强化学习中的不确定性","title":"参考文献","text":"","category":"section"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"[1] Kendall, Alex, and Yarin Gal. \"What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?.\" Advances in neural information processing systems (NeurIPS), 2017.","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"[2] Agarwal, R., Schuurmans, D., and Norouzi, M. An optimistic perspective on offline reinforcement learning. In International Conference on Machine Learning, pp. 104–114. PMLR, 2020.","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"[3] Wu, Yue, et al. \"Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning.\" International Conference on Machine Learning (ICML). PMLR, 2021.","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"[4] Gal, Y. and Ghahramani, Z. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning (ICML), pp.1050–1059, 2016.","category":"page"},{"location":"essays/uncertainty_in_offline_rl/","page":"离线强化学习中的不确定性","title":"离线强化学习中的不确定性","text":"[5] Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S. Stabilizing off-policy q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems (NeurIPS), 2019.","category":"page"},{"location":"#关于","page":"👋 关于","title":"👋 关于","text":"","category":"section"},{"location":"","page":"👋 关于","title":"👋 关于","text":"<img src=\"assets/logo_square.jpg\" width = \"300\" align=center />","category":"page"},{"location":"","page":"👋 关于","title":"👋 关于","text":"\"Good fortune follows upon disaster; disaster lurks within good fortune.\"– Lao Tzu","category":"page"},{"location":"#关于我","page":"👋 关于","title":"关于我","text":"","category":"section"},{"location":"","page":"👋 关于","title":"👋 关于","text":"经历：","category":"page"},{"location":"","page":"👋 关于","title":"👋 关于","text":"2021年暑期短暂在南栖仙策实习。\n2020至今，南京大学人工智能学院研究生在读。研究方向为强化学习，导师：章宗长副教授。\n2016~2020，南京大学化学化工学院本科。","category":"page"},{"location":"","page":"👋 关于","title":"👋 关于","text":"联系：","category":"page"},{"location":"","page":"👋 关于","title":"👋 关于","text":"微信，我老喜欢发朋友圈了……\n知乎，有些东西也喜欢发在知乎上，因为看的人可能会多一点😝\nGithub，目前还是开源组织JuliaReinforcementLearning的成员。","category":"page"},{"location":"","page":"👋 关于","title":"👋 关于","text":"而且我全平台头像几乎都是这个👀","category":"page"},{"location":"#关于本站","page":"👋 关于","title":"关于本站","text":"","category":"section"},{"location":"","page":"👋 关于","title":"👋 关于","text":"首先我要感谢俊哥（我做Julia项目的mentor），给我搭建这个Blog给了很多帮助。实际上，我的Blog就是他的Blog的复制品🤔。还要感谢潇宝，我的很多灵感都来自于他的Github主页。","category":"page"},{"location":"","page":"👋 关于","title":"👋 关于","text":"对于我这种懒人来说，做个Blog非常艰难。不过我确实是想有个地方记录下自己平时科研、写代码和看书的想法，有个Blog或许能鞭策我多写写。不过目前来说因为实习还是啥都没有，等之后闲一点马上就写点东西放上去。","category":"page"},{"location":"","page":"👋 关于","title":"👋 关于","text":"欢迎大家给我友链！欢迎大家给我多点建议！","category":"page"},{"location":"","page":"👋 关于","title":"👋 关于","text":"本站发表的内容默认遵循 CC-BY-4.0。","category":"page"},{"location":"","page":"👋 关于","title":"👋 关于","text":"","category":"page"}]
}
